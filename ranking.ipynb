{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "1b15e060",
   "metadata": {},
   "source": [
    "<h2>CatBoost ranking model</h2>\n",
    "<p>Реализация модели ранжирования товаров с помощью библиотеки <code>CatBoost</code>.\n",
    "Цель — предсказать топ-100 товаров для каждого пользователя.</p>"
   ]
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "<h2>Импорт библиотек</h2>\n",
    "<p>Подключаем Spark, функции для работы с данными, pandas для преобразований,\n",
    "CatBoost для обучения модели ранжирования и scikit-learn для метрики NDCG, catboost для обучения модели ранжирования.\n",
    "Инициализируем <code>findspark</code> для корректного запуска Spark из Python.</p>\n",
    "<hr>"
   ],
   "id": "b6188ad518390f7b"
  },
  {
   "cell_type": "code",
   "id": "5477cea9",
   "metadata": {},
   "source": [
    "from glob import glob\n",
    "import findspark\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import col, sum as spark_sum, when, lit, unix_timestamp, countDistinct, avg, stddev, exp, explode, datediff, current_date, min as spark_min\n",
    "from pyspark.sql import functions as F\n",
    "import pandas as pd\n",
    "from catboost import CatBoostRanker, Pool\n",
    "from sklearn.metrics import ndcg_score\n",
    "import numpy as np\n",
    "from catboost import CatBoost\n",
    "import os\n",
    "findspark.init(\"/opt/spark\")"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "id": "e6a064f4",
   "metadata": {},
   "source": [
    "<h2>Создание SparkSession</h2>\n",
    "<p>Запускаем SparkSession с увеличенными лимитами памяти.\n",
    "В логах оставляем только ошибки и включаем опцию игнорирования повреждённых файлов.</p>\n",
    "<hr>"
   ]
  },
  {
   "cell_type": "code",
   "id": "6b951888",
   "metadata": {},
   "source": [
    "spark = SparkSession.builder.appName(\"OzonApparelAnalysis\") \\\n",
    "    .config(\"spark.driver.memory\", \"8g\").config(\"spark.executor.memory\", \"8g\").getOrCreate() # enabe big data\n",
    "spark.sparkContext.setLogLevel(\"ERROR\") # only errors logged\n",
    "spark.conf.set(\"spark.sql.files.ignoreCorruptFiles\", \"true\")"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "id": "ff1ca2c1",
   "metadata": {},
   "source": [
    "<h2>Загрузка parquet-файлов</h2>\n",
    "<p>Собираем пути и читаем parquet-файлы:\n",
    "<code>orders</code> (заказы), <code>tracker</code> (взаимодействия пользователей),\n",
    "<code>items</code> (товары), <code>categories</code> (иерархия категорий),\n",
    "<code>participants</code> (тестовая выборка).</p>\n",
    "<hr>"
   ]
  },
  {
   "cell_type": "code",
   "id": "dfc332f2",
   "metadata": {},
   "source": [
    "items_files = glob.glob('/srv/data/ml_ozon_recsys_train_final_apparel_items_data/*.parquet')\n",
    "categories_tree = glob.glob('/srv/data/ml_ozon_recsys_train_final_categories_tree/*.parquet')\n",
    "participants_files = glob.glob('/srv/data/ml_ozon_recsys_test_for_participants/*.parquet')\n",
    "orders_files = glob.glob('/srv/data/preprocessed/orders_preprocessed/*.parquet')\n",
    "tracker_files = glob.glob('/srv/data/preprocessed/tracker_preprocessed/*.parquet')\n",
    "test_files = glob.glob('/srv/data/ml_ozon_recsys_test/*.parquet')"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "94d9b572",
   "metadata": {},
   "source": [
    "orders = spark.read.parquet(*orders_files)\n",
    "tracker = spark.read.parquet(*tracker_files)\n",
    "items = spark.read.parquet(*items_files)\n",
    "categories = spark.read.parquet(*categories_tree)"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "id": "e93c3894",
   "metadata": {},
   "source": [
    "<h2>Формирование обучающего датасета</h2>\n",
    "<p>Объединяем заказы, товары и пользовательские действия.\n",
    "Добавляем количество взаимодействий <code>user_item_interactions</code>.\n",
    "Целевая переменная <code>target</code>: 1 — заказ доставлен, иначе 0.</p>\n",
    "<hr>"
   ]
  },
  {
   "cell_type": "code",
   "id": "355f2b4a",
   "metadata": {},
   "source": [
    "train_data = (\n",
    "    orders\n",
    "    .join(items.select(\"item_id\", \"catalogid\"), on=\"item_id\", how=\"left\")\n",
    "    .join(tracker.groupBy(\"user_id\", \"item_id\").count().withColumnRenamed(\"count\", \"user_item_interactions\"),\n",
    "          on=[\"user_id\", \"item_id\"], how=\"left\")\n",
    "    .fillna(0, subset=[\"user_item_interactions\"])\n",
    "    .withColumn(\"target\", F.when(F.col(\"last_status\") == \"delivered_orders\", 1).otherwise(0))\n",
    ")"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "id": "e8abb036",
   "metadata": {},
   "source": [
    "<h2>Проверка распределения партиций</h2>\n",
    "<p>Считаем количество строк в каждой партиции, чтобы проверить баланс нагрузки.</p>\n",
    "<hr>"
   ]
  },
  {
   "cell_type": "code",
   "id": "c9720893",
   "metadata": {},
   "source": [
    "partition_sizes = train_data.rdd.cache().mapPartitions(lambda iterator: [sum(1 for _ in iterator)]).collect()\n",
    "print(partition_sizes)"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "<h2>Проверка распределения партиций</h2>\n",
    "<p>Считаем количество строк в каждой партиции, чтобы проверить баланс нагрузки.</p>\n",
    "<hr>"
   ],
   "id": "e48b4f67e728b93b"
  },
  {
   "cell_type": "code",
   "id": "3570446a",
   "metadata": {},
   "source": [
    "train_data = train_data.repartition(80)"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "<h2>Проверка новых партиций</h2>\n",
    "<p>Снова выводим распределение строк по партициям после перераспределения.</p>\n",
    "<hr>"
   ],
   "id": "c33160ff26afa892"
  },
  {
   "cell_type": "code",
   "id": "194aaf19",
   "metadata": {},
   "source": [
    "partition_sizes = train_data.rdd.cache().mapPartitions(lambda iterator: [sum(1 for _ in iterator)]).collect()\n",
    "print(partition_sizes)"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "e344e8ab",
   "metadata": {},
   "source": [
    "num = train_data.rdd.cache().getNumPartitions()"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "id": "44c93432",
   "metadata": {},
   "source": [
    "<h2>Обучение модели</h2>\n",
    "<p>Обучаем модель CatBoostRanker по партициям. Для каждой партиции создаём <code>Pool</code>\n",
    "с признаками и целевой переменной. Используется функция потерь <code>YetiRank</code>.\n",
    "Сохраняем модель в файл <code>catboost_ranker_final.cbm</code>.</p>\n",
    "<hr>"
   ]
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "prev_model = None\n",
    "\n",
    "for idx in range(num):\n",
    "    def keep_only_i(i):\n",
    "        return lambda part_idx, it: it if part_idx == i else iter([])\n",
    "\n",
    "    partition_df = train_data.rdd.mapPartitionsWithIndex(keep_only_i(idx)).toDF()\n",
    "    train_pd = partition_df.toPandas().sort_values([\"user_id\"])\n",
    "\n",
    "    train_pool = Pool(\n",
    "        data=train_pd[[\"order_weight\", \"user_item_interactions\"]],\n",
    "        label=train_pd[\"target\"],\n",
    "        group_id=train_pd[\"user_id\"]\n",
    "    )\n",
    "\n",
    "    model = CatBoostRanker(\n",
    "        iterations=100,\n",
    "        depth=6,\n",
    "        learning_rate=0.1,\n",
    "        loss_function=\"YetiRank\",\n",
    "        verbose=50\n",
    "    )\n",
    "\n",
    "    if prev_model is None:\n",
    "        model.fit(train_pool)\n",
    "    else:\n",
    "        model.fit(train_pool, init_model=prev_model)\n",
    "\n",
    "    train_preds = model.predict(train_pool)\n",
    "    train_pd[\"score\"] = train_preds\n",
    "\n",
    "    prev_model = model\n",
    "    print(f'{idx}/{num} batches processed')\n",
    "    model.save_model(\"catboost_ranker_final.cbm\")"
   ],
   "id": "83a25c0795e06e20",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "<h2>Загрузка тестовых данных</h2>\n",
    "<p>Читаем тестовую выборку из parquet-файла и сортируем по <code>user_id</code>.</p>\n",
    "<hr>"
   ],
   "id": "860121b6ccfc8520"
  },
  {
   "cell_type": "code",
   "id": "9c60c5db",
   "metadata": {},
   "source": [
    "test_df = spark.read.parquet(\"/srv/data/preprocessed/final_test_only.parquet\").orderBy('user_id')"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "<h2>Перепартиционирование теста</h2>\n",
    "<p>Сохраняем тестовую выборку с разделением по <code>user_id</code> для дальнейшей обработки.</p>\n",
    "<hr>"
   ],
   "id": "882c3f50d57c139c"
  },
  {
   "cell_type": "code",
   "id": "199f2ef8",
   "metadata": {},
   "source": [
    "test_df.write.mode(\"overwrite\").partitionBy(\"user_id\").parquet(\"repartitioned_test.parquet\")"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "<h2>Чтение перепартиционированного теста</h2>\n",
    "<p>Загружаем тестовую выборку с учётом новых партиций.</p>\n",
    "<hr>"
   ],
   "id": "d0c62fabc899965f"
  },
  {
   "cell_type": "code",
   "id": "0a733db3",
   "metadata": {},
   "source": [
    "test_df = spark.read.parquet(\"repartitioned_test.parquet\")"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "<h2>Проверка распределения партиций теста</h2>\n",
    "<p>Считаем количество строк в каждой партиции тестового датасета.</p>\n",
    "<hr>"
   ],
   "id": "6a859bcfab17c028"
  },
  {
   "cell_type": "code",
   "id": "22e480a3",
   "metadata": {},
   "source": [
    "partition_sizes = test_df.rdd.cache().mapPartitions(lambda iterator: [sum(1 for _ in iterator)]).collect()\n",
    "print(partition_sizes)"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "<h2>Количество партиций в тесте</h2>\n",
    "<p>Выводим число партиций в тестовой выборке.</p>\n",
    "<hr>"
   ],
   "id": "b1c7351ac68ac2d9"
  },
  {
   "cell_type": "code",
   "id": "281dc5f4",
   "metadata": {},
   "source": [
    "num = test_df.rdd.cache().getNumPartitions()"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "ef4cfbf3",
   "metadata": {},
   "source": [
    "num"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "<h2>Загрузка модели CatBoost</h2>\n",
    "<p>Загружаем ранее обученную модель CatBoost из файла <code>catboost_ranker_final.cbm</code>.</p>\n",
    "<hr>"
   ],
   "id": "63198529c25b2fe5"
  },
  {
   "cell_type": "code",
   "id": "444e63d8",
   "metadata": {},
   "source": [
    "model = CatBoost()\n",
    "model.load_model('catboost_ranker_final.cbm')"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "a166e8ca",
   "metadata": {},
   "source": [
    "columns = ['user_id'] + [f'item_id_{i}' for i in range(1, 101)]"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "0d98eb4a",
   "metadata": {},
   "source": [
    "columns"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "<h2>Создание CSV для сабмита</h2>\n",
    "<p>Открываем файл для записи результатов и задаём список колонок:\n",
    "<code>user_id</code> и 100 предсказанных <code>item_id</code>.</p>\n",
    "<hr>"
   ],
   "id": "26cf0b7c7ef5e76b"
  },
  {
   "cell_type": "code",
   "id": "d8d455fb",
   "metadata": {},
   "source": "path = 'answer.csv'",
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "8b6eb89e",
   "metadata": {},
   "source": [
    "for idx in range(num):\n",
    "    def keep_only_i(i):\n",
    "        return lambda part_idx, it: it if part_idx == i else iter([])\n",
    "\n",
    "    partition_df = test_df.rdd.cache().mapPartitionsWithIndex(keep_only_i(idx)).toDF()\n",
    "    test_pd = partition_df.toPandas().sort_values([\"user_id\"])\n",
    "    \n",
    "    test_pool = Pool(\n",
    "        data=test_pd[[\"order_weight\", \"user_item_interactions\"]],\n",
    "        group_id=test_pd[\"user_id\"]\n",
    "    )\n",
    "\n",
    "    result = model.predict(test_pool)\n",
    "    test_pd['rank'] = result\n",
    "    rank = test_pd.sort_values(['user_id', 'rank'], ascending=[True, False]).groupby('user_id').head(100)\n",
    "    top_items = rank.groupby('user_id')['item_id'].apply(lambda x: {f'item_id_{i}': val for i, val in enumerate(x)}).unstack(level=1).reset_index()\n",
    "    if not os.path.exists(path):\n",
    "        top_items.to_csv(path, mode='w', index=False)\n",
    "    else:\n",
    "        top_items.to_csv(path, mode='a', header=False, index=False)\n",
    "    print(f'{idx}/{num} processed')"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "4ae7bd61",
   "metadata": {},
   "source": [
    "def process_partition(partition):\n",
    "    print(partition)\n",
    "\n",
    "test_df.foreachPartition(process_partition)"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "e1ad2e53",
   "metadata": {},
   "source": [
    "len(top_items[4368811])"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "95a9cb50",
   "metadata": {},
   "source": [
    "test_pool = Pool(\n",
    "    data=test_pd[[\"order_weight\", \"user_item_interactions\"]],\n",
    "    group_id=test_pd[\"user_id\"]\n",
    ")"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "9a0a47ac",
   "metadata": {},
   "source": [
    "test_preds = model.predict(test_pool)"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "425dc7a1",
   "metadata": {},
   "source": [
    "train_pd[\"score\"] = train_preds\n",
    "\n",
    "ndcg_list = []\n",
    "for user, group in train_pd.groupby(\"user_id\"):\n",
    "    if len(group) < 2:\n",
    "        continue\n",
    "    y_true = group[\"target\"].values.reshape(1, -1)\n",
    "    y_score = group[\"score\"].values.reshape(1, -1)\n",
    "    ndcg = ndcg_score(y_true, y_score, k=100)\n",
    "    ndcg_list.append(ndcg)"
   ],
   "outputs": [],
   "execution_count": null
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
