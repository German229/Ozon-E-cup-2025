{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "e1f24a05",
   "metadata": {},
   "source": [
    "<h2>Импорты и инициализация Spark</h2>\n",
    "<p>Загружаем необходимые библиотеки, настраиваем SparkSession с выделением памяти под драйвер и executor. \n",
    "Устанавливаем уровень логов для удобного чтения.</p>\n",
    "<hr>"
   ]
  },
  {
   "cell_type": "code",
   "id": "bad3b192",
   "metadata": {},
   "source": [
    "import glob\n",
    "import findspark\n",
    "from pyspark.sql import SparkSession, Window\n",
    "from pyspark.sql.functions import col, sum as spark_sum, when, lit, unix_timestamp, countDistinct, avg, stddev, exp, explode, datediff, current_date, min as spark_min\n",
    "from pyspark.sql import functions as F\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import math\n",
    "import os\n",
    "from catboost import CatBoostRanker, Pool\n",
    "from sklearn.metrics import ndcg_score\n",
    "import numpy as np\n",
    "findspark.init(\"/opt/spark\")"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "d0c7f473",
   "metadata": {},
   "source": [
    "spark = SparkSession.builder \\\n",
    "    .appName(\"OzonApparelAnalysis\") \\\n",
    "    .config(\"spark.driver.memory\", \"12g\") \\\n",
    "    .config(\"spark.executor.memory\", \"12g\") \\\n",
    "    .getOrCreate()\n",
    "spark.sparkContext.setLogLevel(\"ERROR\")\n",
    "spark.conf.set(\"spark.sql.files.ignoreCorruptFiles\", \"true\")\n",
    "# spark.sparkContext.setLogLevel(\"INFO\") # расскомментировать чтобы видеть все логи"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "id": "99501bf2",
   "metadata": {},
   "source": [
    "<h2>Загрузка исходных данных</h2>\n",
    "<p>Собираем пути к parquet-файлам и читаем их в DataFrame: заказы, трекер действий пользователей, \n",
    "товары, дерево категорий и тестовый датасет.</p>\n",
    "<hr>"
   ]
  },
  {
   "cell_type": "code",
   "id": "d6b1fed6",
   "metadata": {},
   "source": [
    "items_files = glob.glob('/srv/data/ml_ozon_recsys_train_final_apparel_items_data/*.parquet')\n",
    "categories_tree = glob.glob('/srv/data/ml_ozon_recsys_train_final_categories_tree/*.parquet')\n",
    "participants_files = glob.glob('/srv/data/ml_ozon_recsys_test_for_participants/*.parquet')\n",
    "orders_files = glob.glob('/srv/data/preprocessed/orders_preprocessed/*.parquet')\n",
    "tracker_files = glob.glob('/srv/data/preprocessed/tracker_preprocessed/*.parquet')\n",
    "test_files = glob.glob('/srv/data/ml_ozon_recsys_test/*.parquet')"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "09222a86",
   "metadata": {},
   "source": [
    "orders = spark.read.parquet(*orders_files)\n",
    "tracker = spark.read.parquet(*tracker_files)\n",
    "items = spark.read.parquet(*items_files)\n",
    "categories = spark.read.parquet(*categories_tree)\n",
    "test = spark.read.parquet(*test_files)"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "id": "0be706c4",
   "metadata": {},
   "source": [
    "<h2>Формирование train_df</h2>\n",
    "<p>Создаём базовый тренировочный датасет с пользователями, товарами, весом заказа и количеством взаимодействий. \n",
    "Формируем бинарный target (1 — заказ доставлен, 0 — иначе).</p>\n",
    "<hr>"
   ]
  },
  {
   "cell_type": "code",
   "id": "f550826a",
   "metadata": {},
   "source": [
    "train_df = (\n",
    "    orders\n",
    "    .join(items.select(\"item_id\", \"catalogid\"), on=\"item_id\", how=\"left\")\n",
    "    .join(tracker.groupBy(\"user_id\", \"item_id\").count().withColumnRenamed(\"count\", \"user_item_interactions\"),\n",
    "          on=[\"user_id\", \"item_id\"], how=\"left\")\n",
    "    .fillna(0, subset=[\"user_item_interactions\"])\n",
    "    .withColumn(\"target\", F.when(F.col(\"last_status\") == \"delivered_orders\", 1).otherwise(0))\n",
    ").select(\"user_id\", \"item_id\", \"order_weight\", \"user_item_interactions\")"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "id": "29ffc8e4",
   "metadata": {},
   "source": [
    "<h2>Определение топ-500 популярных товаров</h2>\n",
    "<p>Считаем количество взаимодействий для каждого товара. \n",
    "Делаем две версии: только id товаров (для crossJoin) и с популярностью (для сортировки).</p>\n",
    "<hr>"
   ]
  },
  {
   "cell_type": "code",
   "id": "7e4a1a05",
   "metadata": {},
   "source": [
    "top500_items = (\n",
    "    tracker.groupBy(\"item_id\")\n",
    "           .agg(F.count(\"action_type\").alias(\"amount_of_interactions\"))\n",
    "           .orderBy(F.desc(\"amount_of_interactions\"))\n",
    "           .limit(500)\n",
    "           .select(\"item_id\")\n",
    "           .cache()\n",
    ")\n",
    "\n",
    "top500_pop = (\n",
    "    tracker.groupBy(\"item_id\")\n",
    "           .agg(F.count(\"action_type\").alias(\"amount_of_interactions\"))\n",
    "           .orderBy(F.desc(\"amount_of_interactions\"))\n",
    "           .limit(500)\n",
    "           .cache()\n",
    ")\n"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "id": "d587e647",
   "metadata": {},
   "source": [
    "<h2>Выделение новых пользователей и генерация кандидатов</h2>\n",
    "<p>Находим пользователей из test, которых нет в train. \n",
    "Им присваиваем top-500 популярных товаров с усреднёнными значениями фич.</p>\n",
    "<hr>"
   ]
  },
  {
   "cell_type": "code",
   "id": "a21976ef",
   "metadata": {},
   "source": [
    "new_users = test.distinct().join(train_df.select('user_id').distinct(), how='left_anti', on='user_id').cache()\n",
    "new_users.limit(5).show()"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "d8c7e688",
   "metadata": {},
   "source": [
    "avg_values = train_df.agg(\n",
    "    F.avg(\"order_weight\").alias(\"avg_order_weight\"),\n",
    "    F.avg(\"user_item_interactions\").alias(\"avg_user_item_interactions\")\n",
    ").collect()[0]\n",
    "\n",
    "avg_order_weight = avg_values[\"avg_order_weight\"]\n",
    "avg_user_item_interactions = avg_values[\"avg_user_item_interactions\"]\n",
    "\n",
    "new_users_with_items = (\n",
    "    new_users\n",
    "    .crossJoin(top500_items)\n",
    "    .withColumn(\"order_weight\", F.lit(avg_order_weight))\n",
    "    .withColumn(\"user_item_interactions\", F.lit(avg_user_item_interactions))\n",
    ")\n",
    "\n",
    "print(new_users_with_items.count())\n",
    "new_users_with_items.limit(5).show()"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "id": "cebe5993",
   "metadata": {},
   "source": [
    "<h2>Добивка до 500 товаров для старых пользователей</h2>\n",
    "<p>Формируем кандидатов (user × top500), исключаем уже существующие пары. \n",
    "Для каждого пользователя добавляем недостающие товары (до 500), сортируя по популярности.</p>\n",
    "<hr>"
   ]
  },
  {
   "cell_type": "code",
   "id": "e6625efe",
   "metadata": {},
   "source": [
    "user_item_existing = train_df.select(\"user_id\", \"item_id\").distinct().cache()\n",
    "\n",
    "train_users = train_df.select(\"user_id\").distinct().cache()\n",
    "\n",
    "cross_candidates = train_users.crossJoin(top500_items.select(\"item_id\"))\n",
    "\n",
    "missing_candidates = cross_candidates.join(\n",
    "    user_item_existing, on=[\"user_id\", \"item_id\"], how=\"left_anti\"\n",
    ")\n",
    "\n",
    "\n",
    "user_item_counts = train_df.groupBy(\"user_id\").agg(\n",
    "    F.countDistinct(\"item_id\").alias(\"item_count\")\n",
    ")\n",
    "\n",
    "user_needs = user_item_counts.withColumn(\n",
    "    \"need_count\", F.expr(\"GREATEST(500 - item_count, 0)\")\n",
    ")\n",
    "\n",
    "missing_candidates = missing_candidates.join(user_needs, on=\"user_id\", how=\"inner\")\n",
    "\n",
    "missing_candidates = missing_candidates.join(top500_pop, on=\"item_id\", how=\"left\")\n",
    "\n",
    "w_pop = Window.partitionBy(\"user_id\").orderBy(F.desc(\"amount_of_interactions\"))\n",
    "missing_candidates = (\n",
    "    missing_candidates\n",
    "    .withColumn(\"rn\", F.row_number().over(w_pop))\n",
    "    .filter(F.col(\"rn\") <= F.col(\"need_count\"))\n",
    "    .drop(\"rn\", \"item_count\", \"need_count\", \"amount_of_interactions\")\n",
    "    .withColumn(\"order_weight\", F.lit(avg_order_weight))\n",
    "    .withColumn(\"user_item_interactions\", F.lit(avg_user_item_interactions))\n",
    ")\n",
    "\n"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "id": "0f6936be",
   "metadata": {},
   "source": [
    "<h2>Объединение всех данных</h2>\n",
    "<p>Склеиваем train, новых пользователей и добивку. \n",
    "Затем нормализуем: убираем дубликаты, оставляем ровно 500 товаров на пользователя (с приоритетом его оригинальных покупок).</p>\n",
    "<hr>"
   ]
  },
  {
   "cell_type": "code",
   "id": "090fbcd7",
   "metadata": {},
   "source": [
    "cols = [\"user_id\", \"item_id\", \"order_weight\", \"user_item_interactions\"]\n",
    "\n",
    "train_df_fix = train_df.select(*cols)\n",
    "new_users_with_items_fix = new_users_with_items.select(*cols)\n",
    "missing_candidates_fix = missing_candidates.select(*cols)\n",
    "\n",
    "final_train_pre = (\n",
    "    train_df_fix\n",
    "    .unionByName(new_users_with_items_fix)\n",
    "    .unionByName(missing_candidates_fix)\n",
    "    .dropDuplicates([\"user_id\", \"item_id\"])\n",
    ").cache()\n",
    "\n",
    "original_pairs_flag = (\n",
    "    train_df_fix.select(\"user_id\", \"item_id\")\n",
    "                .withColumn(\"is_original\", F.lit(1))\n",
    "                .distinct()\n",
    ")\n",
    "\n",
    "final_flagged = (\n",
    "    final_train_pre\n",
    "    .join(original_pairs_flag, on=[\"user_id\", \"item_id\"], how=\"left\")\n",
    "    .fillna({\"is_original\": 0})\n",
    "    .join(\n",
    "        top500_pop.select(\"item_id\", \"amount_of_interactions\")\n",
    "                  .withColumnRenamed(\"amount_of_interactions\", \"popularity\"),\n",
    "        on=\"item_id\", how=\"left\"\n",
    "    )\n",
    "    .fillna({\"popularity\": 0})\n",
    ")\n",
    "\n",
    "w_cap = Window.partitionBy(\"user_id\").orderBy(F.desc(\"is_original\"), F.desc(\"popularity\"), F.asc(\"item_id\"))\n",
    "final_train = (\n",
    "    final_flagged\n",
    "    .withColumn(\"rn\", F.row_number().over(w_cap))\n",
    "    .filter(F.col(\"rn\") <= 500)\n",
    "    .select(*cols)\n",
    ").cache()"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "id": "984f56d2",
   "metadata": {},
   "source": [
    "<h2>Санити-проверки</h2>\n",
    "<p>Проверяем общее количество строк, \n",
    "а также что у каждого пользователя ровно по 500 уникальных товаров.</p>\n",
    "<hr>"
   ]
  },
  {
   "cell_type": "code",
   "id": "a051fdd9",
   "metadata": {},
   "source": [
    "# --------- Санити-проверки ----------\n",
    "print(\"Строк в final_train:\", final_train.count())\n",
    "\n",
    "cnts = final_train.groupBy(\"user_id\").agg(F.countDistinct(\"item_id\").alias(\"cnt\"))\n",
    "cnts.agg(\n",
    "    F.min(\"cnt\").alias(\"min\"),\n",
    "    F.avg(\"cnt\").alias(\"avg\"),\n",
    "    F.max(\"cnt\").alias(\"max\")\n",
    ").show()\n",
    "\n",
    "# топ-10 пользователей по числу товаров — должно быть 500 у всех\n",
    "cnts.orderBy(F.desc(\"cnt\")).show(10)\n",
    "\n",
    "final_train.show(5, truncate=False)"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "id": "3f08be9b",
   "metadata": {},
   "source": [
    "<h2>Формирование выборки только для тестовых пользователей</h2>\n",
    "<p>Оставляем только тех пользователей, что встречаются в test. \n",
    "Сохраняем результат в parquet.</p>\n",
    "<hr>"
   ]
  },
  {
   "cell_type": "code",
   "id": "d1a4ec0c",
   "metadata": {},
   "source": [
    "test_user_ids = test.select(\"user_id\").distinct()\n",
    "\n",
    "final_for_test_only = (\n",
    "    final_train\n",
    "    .join(test_user_ids, on=\"user_id\", how=\"inner\")\n",
    ")\n",
    "\n",
    "# final_for_test_only.write.mode(\"overwrite\").parquet(\"/srv/data/preprocessed/final_test_only.parquet\")"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "id": "42b63140",
   "metadata": {},
   "source": [
    "<h2>Финальная проверка</h2>\n",
    "<p>Читаем сохранённый датасет и проверяем количество строк и распределение товаров по пользователям.</p>\n",
    "<hr>"
   ]
  },
  {
   "cell_type": "code",
   "id": "a7c555c2",
   "metadata": {},
   "source": [
    "final_train_loaded = spark.read.parquet(\"/srv/data/preprocessed/final_test_only.parquet\")\n",
    "print(\"Количество строк:\", final_train_loaded.count())\n",
    "final_train_loaded.show(5)"
   ],
   "outputs": [],
   "execution_count": null
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
